---
title: "Exercise Efficacy - a Machine Learning Prospective"
author: "Chuzhe Xiao (Gabriel)"
date: "July 24, 2016"
output: html_document
---

# Synopsis
This report utilized Human Activity Recognition data to predict how well a participant is exercising. First I will load the data and do some preliminary pre-processing. Next, I will divide the training data into a training set and validation set. I will fit two different models using the training set. They are: 1, a generalized linear model, and 2, a random forest model. For the random forest model, the parameter will be chosen using the Out-of-bag (OOB) error.  I will then use the validation set to choose the best performing models.  

# Loading Data
The following code load the data from the web.

```{r,message=FALSE,warning=FALSE}
library(caret)

setwd("C:\\Users\\Gabriel\\Dropbox\\Data Science Coursera\\Practical Machine Learning\\Project")

# download file
if(!file.exists("training.csv")){
        trainingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        download.file(trainingUrl, destfile = "./training.csv")
}
if(!file.exists("testing.csv")){
        testingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
        download.file(testingUrl, destfile = "./testing.csv")
}

## Read data
## Check if variable exist. If so, don't read data in. If not, read data
# cols_type = c("numeric","Factor",rep("numeric",3),"Factor", rep("numeric",153),"Factor")

if(!exists("training")){
        training <- read.csv("training.csv",na.strings = c("NA",""," "))
}
if(!exists("testing")){
        testing <- read.csv("testing.csv",na.strings = c("NA",""," "))
}
```

# Pre-processing

## Drop ID information
By reading the first few rows of the dataset as well as reading the names, we know that the first few columns are information about the participant as well as the time. Clearly we should exclude them from our analysis. We will apply the same method to the testing set.

```{r}
head(training[,1:14])
training = training[,-(1:7)]
testing = testing[,-(1:7)]
```

## Drop Mostly NA Columns
In addition, we can see that there are a lot of columns with mostly NA. We also want to drop these columns.

```{r}
na.num = colSums(is.na(training))
# drop columns with more than 90% NA
testing = testing[,!(na.num>nrow(training)*0.9)]
training_clean = training[,!(na.num>nrow(training)*0.9)] 
dim(training_clean)
```

## Drop mostly zero columns
We make to make sure there are no columns that are mostly zero. I used the `nearZeroVar` function in caret to check this. As it turns out, none of the columns have near zero variance

```{r}
cols = nearZeroVar(training_clean)
length(cols)
```

## Data Partition
Divide the data into training set and validation set. The validation set is used to select model. I also take 
```{r}
set.seed(10)
inTrain = createDataPartition(y=training_clean$classe,p=0.8,list=FALSE)
training_set = training_clean[inTrain,]
validation_set = training_clean[-inTrain,]
```

# Analysis
## Multinomial Logistic Regression
Since the response in this case is a factor variable with more then two values, we can use [Multinomial Logistic Regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression). I used the `multinom` function in the package `nnet` and displayed the in-sample accuracy.

```{r,cache=TRUE}
class(training_set$classe)
library(nnet)
glm_mod = multinom(classe~., data = training_set)
```

### Prediction Performance
Now we can take a look at the prediction performance both in the training set and in the validation set.
```{r}
pred = predict(glm_mod,training_set)
err1 = sum(pred==training_set$classe)/length(pred)
pred_val = predict(glm_mod,validation_set)
err2 = sum(pred_val==validation_set$classe)/length(pred_val)
msg = paste0("In-sample accuracy: ",round(err1,4),", Validation accuracy: ",round(err2,4))
print(msg)
```

## Random Forest
Next, I will use a more sophisticated method, Random Forest, to make prediction. Please refer to page 588 in Elements of Statistical Learning by Hastie, Tibshirani and Friedman for a detailed discussion of random forest algorithms.

Specifically, in R, there are three important parameters. 1. `ntree`: how many trees to grow in the forest. 2, `nodesize`: how many observation in each node. Since this is a classification problem, we will choose 1. 3, `mtry`, how many predictors is chosen in each split to construct the tree. This is usually the most important parameters. Luckily, R provided the `tuneRF` function to for us to choose `mtry`.

Note that `tuneRF` use out-of-bag (OOB) error instead of cross validation error to choose parameters. This is because when building each tree, the sample is bootstrap and we will have sample that are not chosen. These are the out-of-bag sample. These are a natural choice for evaluating the performance of each tree. 

First we need to tune the parameters. `tuneRF` required the input to be matrix, therefore we need to transform the data first. It is clear from the output that we should choose `mtry=7`
```{r,message=FALSE,warning=FALSE}
set.seed(100)
library(randomForest)
response = training_set[,53]
predictors = as.matrix(training_set[,-53])
tuneRF(predictors,response)
```

Now we can fit a random forest model. I choose `ntree=200` to speed up the running time. I also plotted the error as tree size increase. It seems like 200 is more than enough.
```{r}
rffit = randomForest(classe~., data=training_set,mtry=7,nodesize=1,ntree=200)
plot(rffit)
```

### Prediction Performance
Now we can take a look at the prediction performance both in the training set and in the validation set.
```{r}
pred = predict(rffit,training_set)
err1 = sum(pred==training_set$classe)/length(pred)
pred_val = predict(rffit,validation_set)
err2 = sum(pred_val==validation_set$classe)/length(pred_val)
msg = paste0("In-sample accuracy: ",round(err1,4),", Validation accuracy: ",round(err2,4))
print(msg)
```

# Conclusion
Clearly the random forest have much better performance both in the training set and the validation set. Therefore we will choose the random forest model. The estimation of out-of-sample error is the the error in the validation set, or `r round(err2,4)`. 

## Prediction in test set
We can also make a prediction for the test test.
```{r}
test_pred = predict(rffit,testing)
test_pred
```

